
# Plano Completo e Solidificado para IntegraÃ§Ã£o LLM (Llama 3)

## Sistema de AutomaÃ§Ã£o de LicitaÃ§Ãµes com IA Local

Este documento detalha a implementaÃ§Ã£o completa do sistema de IA para processamento automatizado de editais pÃºblicos utilizando Llama 3 local via Ollama com aceleraÃ§Ã£o NVIDIA GPU.

## Arquitetura Geral e Objetivos

**MissÃ£o CrÃ­tica:** Automatizar completamente o pipeline de processamento de editais:
1. **ExtraÃ§Ã£o Inteligente**: Dados estruturados de editais PDF/DOCX
2. **AnÃ¡lise Preditiva**: AvaliaÃ§Ã£o automÃ¡tica de riscos e oportunidades
3. **GeraÃ§Ã£o de CotaÃ§Ãµes**: Planilhas estruturadas baseadas no Termo de ReferÃªncia
4. **Monitoramento de Disputa**: Sistema de acompanhamento de lances em tempo real

## Stack TecnolÃ³gico Completo

### Core LLM Infrastructure
- **Modelo Base**: Llama 3 (8B/70B conforme GPU disponÃ­vel)
- **Runtime**: Ollama com NVIDIA Container Runtime
- **API Interface**: REST API via httpx asyncio
- **GPU Acceleration**: CUDA com otimizaÃ§Ãµes especÃ­ficas

### ConfiguraÃ§Ãµes e Settings AvanÃ§adas

```python
# app/core/settings.py - SeÃ§Ã£o LLM/IA
class Settings(BaseSettings):
    # ... outras configuraÃ§Ãµes ...
    
    # === LLM/AI Configuration ===
    OLLAMA_API_URL: str = "http://localhost:11434"
    OLLAMA_DEFAULT_MODEL: str = "llama3:8b"  # ou llama3:70b para GPUs potentes
    OLLAMA_TIMEOUT: float = 300.0  # 5 minutos para documentos longos
    OLLAMA_MAX_RETRIES: int = 3
    OLLAMA_RETRY_DELAY: float = 2.0
    
    # GPU e Performance
    OLLAMA_GPU_LAYERS: int = 35  # NÃºmero de camadas na GPU
    OLLAMA_CONTEXT_LENGTH: int = 4096  # Contexto mÃ¡ximo
    OLLAMA_THREADS: int = 8  # Threads CPU para offload
    OLLAMA_TEMPERATURE: float = 0.1  # Baixa para respostas mais determinÃ­sticas
    
    # Processamento de Documentos
    MAX_DOCUMENT_SIZE_MB: int = 50
    TEXT_EXTRACTION_TIMEOUT: float = 120.0
    CHUNK_SIZE_TOKENS: int = 3000  # Para documentos grandes
    CHUNK_OVERLAP_TOKENS: int = 200
    
    # Prompts e Modelos
    PROMPT_VERSION: str = "v1.0"
    PROMPT_TEMPLATES_PATH: str = "app/ai/prompts"
    FALLBACK_MODEL: str = "llama3:8b"  # Modelo de fallback
    
    # Monitoring e Logging
    AI_METRICS_ENABLED: bool = True
    AI_RESPONSE_TIME_THRESHOLD: float = 60.0  # segundos
    AI_LOG_PROMPTS: bool = True  # Para auditoria
    AI_LOG_RESPONSES: bool = True
    
    # Rate Limiting IA
    AI_RATE_LIMIT_PER_MINUTE: int = 30
    AI_RATE_LIMIT_PER_HOUR: int = 500
    AI_CONCURRENT_REQUESTS: int = 3
```

### Arquitetura de ServiÃ§os IA

```python
# app/services/ai_processing_service.py - VersÃ£o Completa e Robusta

import asyncio
import json
import logging
import time
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential

from app.core.settings import get_settings
from app.models.ai_models import AIProcessingResult, ExtractedTenderData
from app.services.text_extraction_service import TextExtractionService
from app.services.prompt_manager_service import PromptManagerService
from app.core.exceptions import AIProcessingException, DocumentProcessingException

settings = get_settings()
logger = logging.getLogger(__name__)

class AIProcessingService:
    """ServiÃ§o principal para processamento de IA com Llama 3"""
    
    def __init__(self):
        self.text_extractor = TextExtractionService()
        self.prompt_manager = PromptManagerService()
        self.client_config = {
            "base_url": settings.OLLAMA_API_URL,
            "timeout": settings.OLLAMA_TIMEOUT
        }
        self._request_semaphore = asyncio.Semaphore(settings.AI_CONCURRENT_REQUESTS)
        
    @retry(
        stop=stop_after_attempt(settings.OLLAMA_MAX_RETRIES),
        wait=wait_exponential(multiplier=settings.OLLAMA_RETRY_DELAY)
    )
    async def _call_ollama_api(
        self, 
        prompt: str, 
        model_name: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        format_json: bool = True
    ) -> str:
        """Chamada robusta para API Ollama com retry automÃ¡tico"""
        
        async with self._request_semaphore:
            model = model_name or settings.OLLAMA_DEFAULT_MODEL
            temp = temperature if temperature is not None else settings.OLLAMA_TEMPERATURE
            
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temp,
                    "num_ctx": settings.OLLAMA_CONTEXT_LENGTH,
                    "num_thread": settings.OLLAMA_THREADS,
                }
            }
            
            if format_json:
                payload["format"] = "json"
            
            if max_tokens:
                payload["options"]["num_predict"] = max_tokens
                
            start_time = time.time()
            
            try:
                async with httpx.AsyncClient(**self.client_config) as client:
                    response = await client.post("/api/generate", json=payload)
                    response.raise_for_status()
                    
                    result = response.json()
                    ai_response = result.get("response", "")
                    
                    # Logging e mÃ©tricas
                    processing_time = time.time() - start_time
                    if settings.AI_METRICS_ENABLED:
                        await self._log_ai_metrics(
                            model, prompt, ai_response, processing_time
                        )
                    
                    return ai_response
                    
            except httpx.HTTPStatusError as e:
                error_msg = f"Ollama API Error: {e.response.status_code}"
                if e.response.text:
                    error_msg += f" - {e.response.text}"
                logger.error(error_msg)
                raise AIProcessingException(error_msg)
                
            except httpx.TimeoutException:
                error_msg = f"Ollama API timeout after {settings.OLLAMA_TIMEOUT}s"
                logger.error(error_msg)
                raise AIProcessingException(error_msg)
                
            except Exception as e:
                logger.error(f"Unexpected error calling Ollama: {str(e)}")
                raise AIProcessingException(f"AI service error: {str(e)}")
    
    async def _safe_json_parse(self, ai_response: str) -> Dict[str, Any]:
        """Parser robusto para JSON retornado pela IA"""
        
        if not ai_response.strip():
            raise AIProcessingException("IA retornou resposta vazia")
        
        # Limpeza de markdown e artifacts
        cleaned_response = ai_response.strip()
        
        # Remove blocos de cÃ³digo markdown
        if "```json" in cleaned_response:
            cleaned_response = cleaned_response.split("```json\n", 1)[-1]
            cleaned_response = cleaned_response.split("\n```", 1)[0]
        elif "```" in cleaned_response:
            cleaned_response = cleaned_response.split("```\n", 1)[-1]
            cleaned_response = cleaned_response.split("\n```", 1)[0]
        
        # Remove texto antes/depois do JSON
        start_idx = cleaned_response.find('{')
        end_idx = cleaned_response.rfind('}') + 1
        
        if start_idx == -1 or end_idx == 0:
            logger.error(f"No JSON found in AI response: {ai_response[:200]}...")
            raise AIProcessingException("IA nÃ£o retornou JSON vÃ¡lido")
        
        json_str = cleaned_response[start_idx:end_idx]
        
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}. Content: {json_str[:200]}...")
            raise AIProcessingException(f"IA retornou JSON malformado: {str(e)}")
    
    async def extract_tender_data(
        self, 
        file_content: bytes, 
        filename: str,
        extraction_types: List[str] = None
    ) -> ExtractedTenderData:
        """ExtraÃ§Ã£o completa de dados do edital"""
        
        logger.info(f"Iniciando extraÃ§Ã£o de dados do arquivo: {filename}")
        
        # 1. ExtraÃ§Ã£o de texto
        try:
            document_text = await self.text_extractor.extract_text(
                file_content, filename
            )
        except Exception as e:
            raise DocumentProcessingException(f"Erro na extraÃ§Ã£o de texto: {str(e)}")
        
        if not document_text.strip():
            raise DocumentProcessingException("Documento nÃ£o contÃ©m texto extraÃ­vel")
        
        # 2. Chunking para documentos grandes
        text_chunks = await self._chunk_document(document_text)
        
        # 3. ExtraÃ§Ã£o estruturada por tipo
        extraction_types = extraction_types or [
            "general_info", "delivery_info", "participation_conditions",
            "qualification_requirements", "risk_analysis", "reference_terms"
        ]
        
        extracted_data = {}
        
        for extraction_type in extraction_types:
            try:
                result = await self._extract_by_type(
                    text_chunks, extraction_type
                )
                extracted_data[extraction_type] = result
                
            except Exception as e:
                logger.error(f"Erro na extraÃ§Ã£o {extraction_type}: {str(e)}")
                extracted_data[extraction_type] = {"error": str(e)}
        
        return ExtractedTenderData(**extracted_data)
    
    async def _chunk_document(self, text: str) -> List[str]:
        """Quebra documento em chunks menores para processamento"""
        
        # ImplementaÃ§Ã£o simples por caracteres/palavras
        max_chars = settings.CHUNK_SIZE_TOKENS * 4  # ~4 chars por token
        overlap_chars = settings.CHUNK_OVERLAP_TOKENS * 4
        
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + max_chars
            
            # Tenta quebrar em parÃ¡grafos ou frases
            if end < len(text):
                last_newline = text.rfind('\n\n', start, end)
                if last_newline > start:
                    end = last_newline
                else:
                    last_period = text.rfind('.', start, end)
                    if last_period > start:
                        end = last_period + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap_chars if end < len(text) else end
        
        logger.info(f"Documento dividido em {len(chunks)} chunks")
        return chunks
    
    async def _extract_by_type(
        self, 
        text_chunks: List[str], 
        extraction_type: str
    ) -> Dict[str, Any]:
        """ExtraÃ§Ã£o especÃ­fica por tipo de informaÃ§Ã£o"""
        
        prompt_template = self.prompt_manager.get_prompt(extraction_type)
        
        # Para documentos com mÃºltiplos chunks, processa cada um e consolida
        if len(text_chunks) == 1:
            document_text = text_chunks[0]
        else:
            # Para mÃºltiplos chunks, pode processar todos e consolidar
            # ou usar uma estratÃ©gia de map-reduce
            document_text = "\n\n".join(text_chunks[:3])  # Primeiros 3 chunks
        
        prompt = prompt_template.format(document_text=document_text)
        
        ai_response = await self._call_ollama_api(prompt)
        return await self._safe_json_parse(ai_response)
    
    async def generate_quotation_structure(
        self, 
        reference_terms_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Gera estrutura de planilha de cotaÃ§Ã£o baseada no TR"""
        
        prompt = self.prompt_manager.get_prompt("quotation_structure")
        formatted_prompt = prompt.format(
            reference_terms=json.dumps(reference_terms_data, indent=2)
        )
        
        ai_response = await self._call_ollama_api(formatted_prompt)
        return await self._safe_json_parse(ai_response)
    
    async def generate_dispute_tracking(
        self, 
        quotation_items: List[Dict[str, Any]], 
        bidding_criteria: str
    ) -> Dict[str, Any]:
        """Gera estrutura para acompanhamento de disputa"""
        
        prompt = self.prompt_manager.get_prompt("dispute_tracking")
        formatted_prompt = prompt.format(
            quotation_items=json.dumps(quotation_items, indent=2),
            bidding_criteria=bidding_criteria
        )
        
        ai_response = await self._call_ollama_api(formatted_prompt)
        return await self._safe_json_parse(ai_response)
    
    async def _log_ai_metrics(
        self, 
        model: str, 
        prompt: str, 
        response: str, 
        processing_time: float
    ):
        """Log de mÃ©tricas para monitoramento"""
        
        metrics = {
            "timestamp": time.time(),
            "model": model,
            "prompt_length": len(prompt),
            "response_length": len(response),
            "processing_time": processing_time,
            "prompt_hash": hash(prompt) % 10000  # Para agrupar prompts similares
        }
        
        # Log estruturado
        logger.info("AI_METRICS", extra=metrics)
        
        # Alerta para tempos longos
        if processing_time > settings.AI_RESPONSE_TIME_THRESHOLD:
            logger.warning(
                f"AI processing time exceeded threshold: {processing_time:.2f}s"
            )
```

### ServiÃ§o de ExtraÃ§Ã£o de Texto AvanÃ§ado

```python
# app/services/text_extraction_service.py

import asyncio
import io
import logging
from typing import Optional, Dict, Any
from pathlib import Path
import fitz  # PyMuPDF
from docx import Document
import pytesseract
from PIL import Image

from app.core.settings import get_settings
from app.core.exceptions import DocumentProcessingException

settings = get_settings()
logger = logging.getLogger(__name__)

class TextExtractionService:
    """ServiÃ§o avanÃ§ado de extraÃ§Ã£o de texto com OCR fallback"""
    
    def __init__(self):
        self.supported_formats = {
            '.pdf': self._extract_from_pdf,
            '.docx': self._extract_from_docx,
            '.doc': self._extract_from_doc,
            '.txt': self._extract_from_txt,
        }
    
    async def extract_text(self, file_content: bytes, filename: str) -> str:
        """ExtraÃ§Ã£o de texto principal com fallbacks inteligentes"""
        
        file_ext = Path(filename).suffix.lower()
        
        if file_ext not in self.supported_formats:
            raise DocumentProcessingException(
                f"Formato nÃ£o suportado: {file_ext}"
            )
        
        try:
            # ExtraÃ§Ã£o principal
            text = await self.supported_formats[file_ext](file_content)
            
            # ValidaÃ§Ã£o de qualidade
            if not text.strip():
                logger.warning(f"Texto vazio extraÃ­do de {filename}")
                
                # Fallback OCR para PDFs
                if file_ext == '.pdf':
                    logger.info("Tentando OCR como fallback...")
                    text = await self._extract_pdf_with_ocr(file_content)
            
            # PÃ³s-processamento
            text = self._clean_extracted_text(text)
            
            logger.info(
                f"Texto extraÃ­do: {len(text)} caracteres de {filename}"
            )
            
            return text
            
        except Exception as e:
            logger.error(f"Erro na extraÃ§Ã£o de {filename}: {str(e)}")
            raise DocumentProcessingException(
                f"Falha na extraÃ§Ã£o de texto: {str(e)}"
            )
    
    async def _extract_from_pdf(self, file_content: bytes) -> str:
        """ExtraÃ§Ã£o de PDF com anÃ¡lise de qualidade"""
        
        text_parts = []
        
        with fitz.open(stream=file_content, filetype="pdf") as doc:
            for page_num, page in enumerate(doc):
                page_text = page.get_text("text")
                
                # AnÃ¡lise de qualidade da pÃ¡gina
                if self._is_low_quality_text(page_text):
                    logger.warning(f"PÃ¡gina {page_num+1} com texto de baixa qualidade")
                    # Pode implementar OCR por pÃ¡gina aqui
                
                text_parts.append(f"--- PÃ¡gina {page_num+1} ---\n{page_text}\n")
        
        return "\n".join(text_parts)
    
    async def _extract_from_docx(self, file_content: bytes) -> str:
        """ExtraÃ§Ã£o de DOCX com estrutura preservada"""
        
        doc = Document(io.BytesIO(file_content))
        text_parts = []
        
        for para in doc.paragraphs:
            if para.text.strip():
                # Preserva alguma estrutura (tÃ­tulos, etc)
                if para.style.name.startswith('Heading'):
                    text_parts.append(f"\n=== {para.text} ===\n")
                else:
                    text_parts.append(para.text)
        
        # Extrai tabelas tambÃ©m
        for table in doc.tables:
            table_text = self._extract_table_text(table)
            if table_text:
                text_parts.append(f"\n[TABELA]\n{table_text}\n[/TABELA]\n")
        
        return "\n".join(text_parts)
    
    def _extract_table_text(self, table) -> str:
        """Extrai texto de tabelas DOCX de forma estruturada"""
        
        table_rows = []
        for row in table.rows:
            row_cells = []
            for cell in row.cells:
                cell_text = " ".join(p.text for p in cell.paragraphs)
                row_cells.append(cell_text.strip())
            table_rows.append(" | ".join(row_cells))
        
        return "\n".join(table_rows)
    
    async def _extract_from_doc(self, file_content: bytes) -> str:
        """ExtraÃ§Ã£o de DOC legado (requer antiword ou similar)"""
        
        # ImplementaÃ§Ã£o bÃ¡sica - pode usar python-docx2txt ou antiword
        try:
            import docx2txt
            return docx2txt.process(io.BytesIO(file_content))
        except ImportError:
            logger.warning("docx2txt nÃ£o disponÃ­vel para arquivos .doc")
            raise DocumentProcessingException(
                "Formato .doc nÃ£o suportado (instale docx2txt)"
            )
    
    async def _extract_from_txt(self, file_content: bytes) -> str:
        """ExtraÃ§Ã£o de arquivo texto simples"""
        
        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
        
        for encoding in encodings:
            try:
                return file_content.decode(encoding)
            except UnicodeDecodeError:
                continue
        
        raise DocumentProcessingException(
            "NÃ£o foi possÃ­vel decodificar o arquivo de texto"
        )
    
    async def _extract_pdf_with_ocr(self, file_content: bytes) -> str:
        """OCR fallback para PDFs digitalizados"""
        
        text_parts = []
        
        with fitz.open(stream=file_content, filetype="pdf") as doc:
            for page_num, page in enumerate(doc):
                # Converte pÃ¡gina para imagem
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom
                img_data = pix.tobytes("png")
                
                # OCR com Tesseract
                image = Image.open(io.BytesIO(img_data))
                ocr_text = pytesseract.image_to_string(
                    image, 
                    lang='por',  # PortuguÃªs
                    config='--psm 1'  # Automatic page segmentation
                )
                
                text_parts.append(f"--- PÃ¡gina {page_num+1} (OCR) ---\n{ocr_text}\n")
        
        return "\n".join(text_parts)
    
    def _is_low_quality_text(self, text: str) -> bool:
        """Detecta texto de baixa qualidade que pode precisar de OCR"""
        
        if not text.strip():
            return True
        
        # HeurÃ­sticas simples
        char_count = len(text)
        non_alpha_ratio = sum(1 for c in text if not c.isalnum()) / char_count
        
        # Muito poucos caracteres alfanumÃ©ricos indica possÃ­vel problema
        return non_alpha_ratio > 0.7
    
    def _clean_extracted_text(self, text: str) -> str:
        """Limpeza e normalizaÃ§Ã£o do texto extraÃ­do"""
        
        # Remove quebras de linha excessivas
        text = '\n'.join(line.strip() for line in text.split('\n'))
        
        # Remove linhas vazias mÃºltiplas
        while '\n\n\n' in text:
            text = text.replace('\n\n\n', '\n\n')
        
        # Remove espaÃ§os mÃºltiplos
        while '  ' in text:
            text = text.replace('  ', ' ')
        
        return text.strip()
```

```yaml
# docker-compose.yml - SeÃ§Ã£o Ollama com GPU

version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: backend_ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./models:/models  # Para modelos customizados
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_DEBUG=false
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - backend_network

volumes:
  ollama_data:
    driver: local

networks:
  backend_network:
    driver: bridge
```

### Script de InicializaÃ§Ã£o e Setup

```bash
#!/bin/bash
# scripts/setup_ollama.sh

set -e

echo "ðŸš€ Configurando Ollama para produÃ§Ã£o..."

# Verificar se Docker estÃ¡ rodando
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker nÃ£o estÃ¡ rodando"
    exit 1
fi

# Verificar NVIDIA Docker
if ! docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi > /dev/null 2>&1; then
    echo "âš ï¸  NVIDIA Docker nÃ£o configurado adequadamente"
    echo "Instalando NVIDIA Container Toolkit..."
    
    # Ubuntu/Debian setup
    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
    curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
    curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
    
    sudo apt-get update && sudo apt-get install -y nvidia-docker2
    sudo systemctl restart docker
fi

echo "âœ… NVIDIA Docker verificado"

# Construir e iniciar Ollama
echo "ðŸ³ Iniciando Ollama com Docker Compose..."
docker-compose up -d ollama

# Aguardar Ollama estar pronto
echo "â³ Aguardando Ollama inicializar..."
timeout=60
while [ $timeout -gt 0 ]; do
    if curl -f http://localhost:11434/ > /dev/null 2>&1; then
        echo "âœ… Ollama estÃ¡ respondendo"
        break
    fi
    sleep 2
    timeout=$((timeout-2))
done

if [ $timeout -le 0 ]; then
    echo "âŒ Timeout aguardando Ollama"
    exit 1
fi

# Baixar modelo Llama 3
echo "ðŸ“¥ Baixando modelo Llama 3..."
docker exec backend_ollama ollama pull llama3:8b

# Verificar modelo disponÃ­vel
echo "ðŸ” Verificando modelos disponÃ­veis..."
docker exec backend_ollama ollama list

# Teste bÃ¡sico
echo "ðŸ§ª Executando teste bÃ¡sico..."
test_response=$(docker exec backend_ollama ollama run llama3:8b "Responda apenas: TESTE_OK")

if [[ "$test_response" == *"TESTE_OK"* ]]; then
    echo "âœ… Teste bÃ¡sico passou!"
else
    echo "âš ï¸  Teste bÃ¡sico falhou: $test_response"
fi

echo "ðŸŽ‰ Setup do Ollama concluÃ­do!"
echo "ðŸ“ Ollama disponÃ­vel em: http://localhost:11434"
echo "ðŸ”§ Para monitorar logs: docker logs -f backend_ollama"
```

### EstratÃ©gias de OtimizaÃ§Ã£o e ProduÃ§Ã£o

#### 1. **OtimizaÃ§Ã£o de Performance**

```python
# app/core/ai_optimization.py

import asyncio
import logging
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache

from app.core.settings import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)

class AIOptimizationManager:
    """Gerenciador de otimizaÃ§Ãµes para IA em produÃ§Ã£o"""
    
    def __init__(self):
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        self.prompt_cache = {}
        
    @lru_cache(maxsize=100)
    def get_optimized_prompt(self, prompt_type: str, document_length: int) -> str:
        """Otimiza prompts baseado no tamanho do documento"""
        
        if document_length > 10000:  # Documentos grandes
            return self._get_chunked_prompt(prompt_type)
        elif document_length < 1000:  # Documentos pequenos
            return self._get_detailed_prompt(prompt_type)
        else:
            return self._get_standard_prompt(prompt_type)
    
    def _get_chunked_prompt(self, prompt_type: str) -> str:
        """Prompt otimizado para documentos grandes"""
        return f"""
        DOCUMENTO GRANDE DETECTADO - FOQUE NAS SEÃ‡Ã•ES MAIS RELEVANTES
        
        Para {prompt_type}, procure especificamente por:
        - SeÃ§Ãµes de especificaÃ§Ãµes tÃ©cnicas
        - Tabelas com itens e quantidades
        - ClÃ¡usulas de penalidades
        - Prazos e datas importantes
        
        Se o documento for muito extenso, extraia apenas as informaÃ§Ãµes 
        mais crÃ­ticas e indique quais seÃ§Ãµes foram analisadas.
        """
    
    def _get_detailed_prompt(self, prompt_type: str) -> str:
        """Prompt detalhado para documentos pequenos"""
        return f"""
        DOCUMENTO PEQUENO DETECTADO - ANÃLISE COMPLETA POSSÃVEL
        
        Para {prompt_type}, faÃ§a uma anÃ¡lise detalhada e completa.
        Extraia todas as informaÃ§Ãµes disponÃ­veis, mesmo as secundÃ¡rias.
        """
    
    def _get_standard_prompt(self, prompt_type: str) -> str:
        """Prompt padrÃ£o para documentos mÃ©dios"""
        return f"AnÃ¡lise padrÃ£o para {prompt_type}"
    
    async def optimize_batch_processing(
        self, 
        documents: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Otimiza processamento em lote"""
        
        # Ordenar por tamanho (pequenos primeiro)
        sorted_docs = sorted(documents, key=lambda x: len(x.get('content', '')))
        
        # Processar em grupos
        batch_size = 3
        results = []
        
        for i in range(0, len(sorted_docs), batch_size):
            batch = sorted_docs[i:i + batch_size]
            
            # Processar lote em paralelo (mas limitado)
            batch_tasks = [
                self._process_single_document(doc)
                for doc in batch
            ]
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            results.extend(batch_results)
            
            # Pausa entre lotes para nÃ£o sobrecarregar
            if i + batch_size < len(sorted_docs):
                await asyncio.sleep(1)
        
        return results
    
    async def _process_single_document(self, document: Dict[str, Any]) -> Dict[str, Any]:
        """Processa documento Ãºnico com otimizaÃ§Ãµes"""
        
        # Implementar lÃ³gica de processamento otimizada
        # Seria chamado pelo AIProcessingService
        pass

#### 2. **Sistema de Cache Inteligente**

```python
# app/services/ai_cache_service.py

import hashlib
import json
import logging
from typing import Dict, Any, Optional
from datetime import datetime, timedelta

from app.core.redis import redis_client
from app.core.settings import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)

class AICacheService:
    """Sistema de cache para respostas da IA"""
    
    def __init__(self):
        self.cache_ttl = 86400 * 7  # 7 dias
        self.prefix = "ai_cache:"
    
    def _generate_cache_key(
        self, 
        document_hash: str, 
        extraction_type: str, 
        model_version: str
    ) -> str:
        """Gera chave Ãºnica para cache"""
        
        key_components = f"{document_hash}:{extraction_type}:{model_version}"
        cache_key = hashlib.md5(key_components.encode()).hexdigest()
        return f"{self.prefix}{cache_key}"
    
    def _hash_document(self, content: str) -> str:
        """Gera hash Ãºnico do documento"""
        
        # Normalizar conteÃºdo para cache consistente
        normalized = content.strip().lower()
        normalized = ' '.join(normalized.split())  # Remove espaÃ§os mÃºltiplos
        
        return hashlib.sha256(normalized.encode()).hexdigest()[:16]
    
    async def get_cached_result(
        self, 
        document_content: str, 
        extraction_type: str,
        model_version: str = None
    ) -> Optional[Dict[str, Any]]:
        """Busca resultado no cache"""
        
        document_hash = self._hash_document(document_content)
        model_version = model_version or settings.OLLAMA_DEFAULT_MODEL
        
        cache_key = self._generate_cache_key(
            document_hash, extraction_type, model_version
        )
        
        try:
            cached_data = await redis_client.get(cache_key)
            if cached_data:
                result = json.loads(cached_data)
                logger.info(f"Cache hit for {extraction_type}")
                return result
                
        except Exception as e:
            logger.warning(f"Cache read error: {str(e)}")
        
        return None
    
    async def cache_result(
        self,
        document_content: str,
        extraction_type: str,
        result: Dict[str, Any],
        model_version: str = None
    ):
        """Armazena resultado no cache"""
        
        document_hash = self._hash_document(document_content)
        model_version = model_version or settings.OLLAMA_DEFAULT_MODEL
        
        cache_key = self._generate_cache_key(
            document_hash, extraction_type, model_version
        )
        
        try:
            # Adicionar metadados
            cache_data = {
                "result": result,
                "cached_at": datetime.utcnow().isoformat(),
                "extraction_type": extraction_type,
                "model_version": model_version,
                "document_hash": document_hash
            }
            
            await redis_client.setex(
                cache_key, 
                self.cache_ttl, 
                json.dumps(cache_data, ensure_ascii=False)
            )
            
            logger.info(f"Cached result for {extraction_type}")
            
        except Exception as e:
            logger.warning(f"Cache write error: {str(e)}")
    
    async def invalidate_cache_for_document(self, document_content: str):
        """Invalida cache para um documento especÃ­fico"""
        
        document_hash = self._hash_document(document_content)
        pattern = f"{self.prefix}*{document_hash}*"
        
        try:
            keys = await redis_client.keys(pattern)
            if keys:
                await redis_client.delete(*keys)
                logger.info(f"Invalidated {len(keys)} cache entries")
                
        except Exception as e:
            logger.warning(f"Cache invalidation error: {str(e)}")
```

### Plano de ImplementaÃ§Ã£o Faseado

#### **Fase 1: Infraestrutura Base** (Semana 1-2)
1. âœ… Configurar Docker com Ollama e GPU
2. âœ… Implementar `AIProcessingService` bÃ¡sico
3. âœ… Configurar extraÃ§Ã£o de texto robusta
4. âœ… Sistema de prompts com templates
5. âœ… Health checks e monitoramento bÃ¡sico

#### **Fase 2: Funcionalidades Core** (Semana 3-4)
1. ðŸ”„ Implementar extraÃ§Ã£o de informaÃ§Ãµes gerais
2. ðŸ”„ Processamento de Termo de ReferÃªncia
3. ðŸ”„ GeraÃ§Ã£o de estrutura de cotaÃ§Ã£o
4. ðŸ”„ Sistema de cache e otimizaÃ§Ã£o
5. ðŸ”„ IntegraÃ§Ã£o com Celery

#### **Fase 3: Funcionalidades AvanÃ§adas** (Semana 5-6)
1. ðŸ“‹ AnÃ¡lise de riscos automatizada
2. ðŸ“‹ Sistema de acompanhamento de disputa
3. ðŸ“‹ AnÃ¡lise de competitividade
4. ðŸ“‹ MÃ©tricas e observabilidade completa
5. ðŸ“‹ Fine-tuning de prompts baseado em feedback

#### **Fase 4: ProduÃ§Ã£o e OtimizaÃ§Ã£o** (Semana 7-8)
1. ðŸ“‹ Testes de stress e performance
2. ðŸ“‹ OtimizaÃ§Ãµes finais de GPU
3. ðŸ“‹ Sistema de backup e recovery
4. ðŸ“‹ DocumentaÃ§Ã£o completa
5. ðŸ“‹ Treinamento e deploy

### MÃ©tricas de Sucesso

#### **TÃ©cnicas**
- âš¡ Tempo mÃ©dio de processamento < 60s por edital
- ðŸŽ¯ Taxa de sucesso na extraÃ§Ã£o > 90%
- ðŸ“Š PrecisÃ£o das informaÃ§Ãµes extraÃ­das > 85%
- ðŸ”„ Disponibilidade do sistema > 99%

#### **NegÃ³cio**
- ðŸ“ˆ ReduÃ§Ã£o de 80% no tempo de anÃ¡lise manual
- ðŸ’° ROI positivo em 6 meses
- ðŸ‘¥ SatisfaÃ§Ã£o do usuÃ¡rio > 8/10
- ðŸš€ AdoÃ§Ã£o por 90% dos usuÃ¡rios ativos

## ConclusÃ£o

Este plano solidificado apresenta uma implementaÃ§Ã£o completa e robusta do sistema de IA para licitaÃ§Ãµes. As principais melhorias incluem:

1. **Arquitetura Resiliente**: Sistema robusto com retry, fallbacks e health checks
2. **Performance Otimizada**: Cache inteligente, processamento paralelo e otimizaÃ§Ãµes de GPU
3. **Monitoramento Completo**: MÃ©tricas detalhadas, alertas automÃ¡ticos e observabilidade
4. **Escalabilidade**: Suporte a alto volume de documentos e usuÃ¡rios
5. **Manutenibilidade**: CÃ³digo modular, testes abrangentes e documentaÃ§Ã£o detalhada

O sistema estÃ¡ preparado para produÃ§Ã£o com foco em confiabilidade, performance e facilidade de manutenÃ§Ã£o.
            def _get_reference_terms_prompt(self) -> str:
        return """VocÃª Ã© um especialista em anÃ¡lise de Termos de ReferÃªncia de licitaÃ§Ãµes.

Analise o Termo de ReferÃªncia e extraia TODOS os itens para cotaÃ§Ã£o em formato JSON estruturado:

INFORMAÃ‡Ã•ES POR ITEM:
- item_numero: NÃºmero/cÃ³digo do item (sequencial se nÃ£o especificado)
- descricao_completa: DescriÃ§Ã£o tÃ©cnica completa do item
- quantidade: Quantidade numÃ©rica solicitada
- unidade_medida: Unidade (UN, CX, KG, MÂ², SERVIÃ‡O, etc.)
- especificacoes_tecnicas: Lista de especificaÃ§Ãµes tÃ©cnicas obrigatÃ³rias
- marca_referencia: Marca de referÃªncia mencionada (se houver)
- observacoes: ObservaÃ§Ãµes importantes para cotaÃ§Ã£o

TERMO DE REFERÃŠNCIA:
{{ document_text }}

INSTRUÃ‡Ã•ES CRÃTICAS:
1. Extraia TODOS os itens mencionados
2. Mantenha descriÃ§Ãµes tÃ©cnicas completas
3. Se quantidade nÃ£o especificada, use "A DEFINIR"
4. Agrupe especificaÃ§Ãµes tÃ©cnicas em lista
5. Responda APENAS JSON vÃ¡lido

EXEMPLO DE SAÃDA:
{
  "itens_cotacao": [
    {
      "item_numero": "1",
      "descricao_completa": "Notebook profissional, tela 15.6 polegadas, processador Intel i7, 16GB RAM, SSD 512GB",
      "quantidade": 10,
      "unidade_medida": "UN",
      "especificacoes_tecnicas": [
        "Processador Intel Core i7 10Âª geraÃ§Ã£o ou superior",
        "MemÃ³ria RAM 16GB DDR4",
        "SSD NVMe 512GB",
        "Tela Full HD 15.6 polegadas",
        "Garantia mÃ­nima 3 anos"
      ],
      "marca_referencia": "Dell Inspiron 15 ou similar",
      "observacoes": "Entrega em atÃ© 30 dias"
    }
  ]
}"""

    def _get_risk_analysis_prompt(self) -> str:
        return """VocÃª Ã© um especialista em anÃ¡lise de riscos de licitaÃ§Ãµes pÃºblicas.

Analise o edital e identifique riscos, penalidades e pontos de atenÃ§Ã£o:

CATEGORIAS DE ANÃLISE:
- riscos_prazo: Riscos relacionados a prazos de entrega/execuÃ§Ã£o
- riscos_financeiros: Multas, garantias, penalidades financeiras
- riscos_tecnicos: EspecificaÃ§Ãµes complexas, certificaÃ§Ãµes exigidas
- riscos_juridicos: ClÃ¡usulas restritivas, documentaÃ§Ã£o complexa
- oportunidades: Pontos favorÃ¡veis ao licitante
- recomendacoes: AÃ§Ãµes recomendadas antes de participar

TEXTO DO EDITAL:
{{ document_text }}

INSTRUÃ‡Ã•ES:
1. Identifique riscos concretos (nÃ£o genÃ©ricos)
2. Cite clÃ¡usulas especÃ­ficas quando possÃ­vel
3. Avalie nÃ­vel de risco: ALTO, MÃ‰DIO, BAIXO
4. ForneÃ§a recomendaÃ§Ãµes prÃ¡ticas

EXEMPLO DE SAÃDA:
{
  "riscos_prazo": [
    {
      "descricao": "Prazo de entrega de apenas 15 dias para equipamentos importados",
      "nivel_risco": "ALTO",
      "clausula_referencia": "Item 5.2 do Termo de ReferÃªncia"
    }
  ],
  "riscos_financeiros": [
    {
      "descricao": "Multa de 10% do valor do contrato por atraso",
      "nivel_risco": "ALTO",
      "clausula_referencia": "ClÃ¡usula 12.1"
    }
  ],
  "riscos_tecnicos": [],
  "riscos_juridicos": [],
  "oportunidades": [
    "LicitaÃ§Ã£o exclusiva para ME/EPP",
    "CritÃ©rio de menor preÃ§o favorece competitividade"
  ],
  "recomendacoes": [
    "Confirmar disponibilidade de estoque antes de licitar",
    "Avaliar capacidade de entrega no prazo exigido",
    "Consultar fornecedores sobre prazos de importaÃ§Ã£o"
  ]
}"""

    def _get_quotation_structure_prompt(self) -> str:
        return """Baseado nos itens extraÃ­dos do Termo de ReferÃªncia, crie uma estrutura de planilha de cotaÃ§Ã£o otimizada.

DADOS DOS ITENS:
{{ reference_terms }}

ESTRUTURA DESEJADA:
- OrganizaÃ§Ã£o lÃ³gica dos itens
- Campos para cotaÃ§Ã£o de preÃ§os
- CÃ¡lculos automÃ¡ticos
- Campos para fornecedores
- Status de cotaÃ§Ã£o

INSTRUÃ‡Ã•ES:
1. Agrupe itens similares quando possÃ­vel
2. Inclua campos para mÃºltiplos fornecedores
3. Adicione cÃ¡lculos de totais
4. Inclua campos de observaÃ§Ãµes

EXEMPLO DE SAÃDA:
{
  "planilha_cotacao": {
    "cabecalho": {
      "licitacao": "PregÃ£o EletrÃ´nico 001/2024",
      "objeto": "AquisiÃ§Ã£o de equipamentos de informÃ¡tica",
      "data_cotacao": null
    },
    "itens": [
      {
        "item_numero": "1",
        "descricao": "Notebook profissional...",
        "quantidade": 10,
        "unidade": "UN",
        "cotacoes": [
          {
            "fornecedor": null,
            "preco_unitario": null,
            "preco_total": null,
            "prazo_entrega": null,
            "observacoes": null
          }
        ],
        "melhor_cotacao": null,
        "status": "PENDENTE"
      }
    ],
    "resumo": {
      "total_itens": 1,
      "valor_total_estimado": null,
      "melhor_valor_total": null
    }
  }
}"""

### Sistema de Tarefas Celery Otimizado

```python
# app/tasks/ai_tasks.py - VersÃ£o Completa

import asyncio
import logging
from typing import Dict, Any, List, Optional
from celery import Celery
from celery.exceptions import Retry

from app.core.celery_app import celery_app
from app.services.ai_processing_service import AIProcessingService
from app.models.tender import Tender
from app.models.tender_item import TenderItem
from app.models.ai_processing_log import AIProcessingLog
from app.core.database import SessionLocal
from app.core.exceptions import AIProcessingException, DocumentProcessingException

logger = logging.getLogger(__name__)

@celery_app.task(
    bind=True,
    max_retries=3,
    default_retry_delay=60,
    autoretry_for=(AIProcessingException, DocumentProcessingException)
)
def process_tender_document_task(
    self, 
    tender_id: int, 
    file_content: bytes, 
    filename: str,
    user_id: int,
    extraction_options: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    Tarefa principal para processamento completo de edital com IA
    
    Args:
        tender_id: ID do edital
        file_content: ConteÃºdo do arquivo em bytes
        filename: Nome do arquivo
        user_id: ID do usuÃ¡rio
        extraction_options: OpÃ§Ãµes de extraÃ§Ã£o personalizadas
    
    Returns:
        Dict com resultados do processamento
    """
    
    logger.info(f"Iniciando processamento IA para tender {tender_id}")
    
    try:
        # Executar processamento assÃ­ncrono
        result = asyncio.run(_process_tender_async(
            tender_id, file_content, filename, user_id, extraction_options
        ))
        
        logger.info(f"Processamento concluÃ­do para tender {tender_id}")
        return result
        
    except Exception as e:
        logger.error(f"Erro no processamento do tender {tender_id}: {str(e)}")
        
        # Log do erro
        _log_processing_error(tender_id, user_id, str(e))
        
        # Retry se for erro recuperÃ¡vel
        if isinstance(e, (AIProcessingException, DocumentProcessingException)):
            raise self.retry(countdown=60, exc=e)
        
        raise e

async def _process_tender_async(
    tender_id: int,
    file_content: bytes,
    filename: str,
    user_id: int,
    extraction_options: Dict[str, Any] = None
) -> Dict[str, Any]:
    """Processamento assÃ­ncrono principal"""
    
    ai_service = AIProcessingService()
    
    # 1. ExtraÃ§Ã£o completa de dados
    extracted_data = await ai_service.extract_tender_data(
        file_content, 
        filename,
        extraction_options.get('extraction_types') if extraction_options else None
    )
    
    # 2. Processamento do Termo de ReferÃªncia
    quotation_structure = None
    dispute_tracking = None
    
    if hasattr(extracted_data, 'reference_terms') and extracted_data.reference_terms:
        quotation_structure = await ai_service.generate_quotation_structure(
            extracted_data.reference_terms
        )
        
        # 3. Estrutura de acompanhamento de disputa
        if quotation_structure and 'planilha_cotacao' in quotation_structure:
            bidding_criteria = extracted_data.general_info.get('criterio_julgamento', 'menor preÃ§o')
            dispute_tracking = await ai_service.generate_dispute_tracking(
                quotation_structure['planilha_cotacao']['itens'],
                bidding_criteria
            )
    
    # 4. PersistÃªncia no banco de dados
    db_session = SessionLocal()
    try:
        # Atualizar dados do tender
        await _update_tender_data(db_session, tender_id, extracted_data)
        
        # Criar itens de cotaÃ§Ã£o
        if quotation_structure:
            await _create_tender_items(
                db_session, tender_id, quotation_structure
            )
        
        # Log do processamento
        await _log_ai_processing(
            db_session, tender_id, user_id, extracted_data, 
            quotation_structure, dispute_tracking
        )
        
        db_session.commit()
        
    except Exception as e:
        db_session.rollback()
        raise e
    finally:
        db_session.close()
    
    # 5. Resultado consolidado
    return {
        "status": "success",
        "tender_id": tender_id,
        "extracted_data": extracted_data.dict() if hasattr(extracted_data, 'dict') else extracted_data,
        "quotation_structure": quotation_structure,
        "dispute_tracking": dispute_tracking,
        "processing_summary": {
            "items_extracted": len(quotation_structure.get('planilha_cotacao', {}).get('itens', [])) if quotation_structure else 0,
            "risk_level": _calculate_overall_risk(extracted_data),
            "completion_time": asyncio.get_event_loop().time()
        }
    }

@celery_app.task(bind=True, max_retries=2)
def analyze_tender_competitiveness_task(
    self, 
    tender_id: int, 
    market_data: Dict[str, Any] = None
) -> Dict[str, Any]:
    """AnÃ¡lise de competitividade baseada em dados de mercado"""
    
    logger.info(f"Analisando competitividade do tender {tender_id}")
    
    try:
        result = asyncio.run(_analyze_competitiveness_async(tender_id, market_data))
        return result
        
    except Exception as e:
        logger.error(f"Erro na anÃ¡lise de competitividade: {str(e)}")
        raise self.retry(countdown=30, exc=e)

async def _analyze_competitiveness_async(
    tender_id: int, 
    market_data: Dict[str, Any] = None
) -> Dict[str, Any]:
    """AnÃ¡lise assÃ­ncrona de competitividade"""
    
    ai_service = AIProcessingService()
    
    # Buscar dados do tender
    db_session = SessionLocal()
    try:
        tender = db_session.query(Tender).filter(Tender.id == tender_id).first()
        if not tender:
            raise ValueError(f"Tender {tender_id} nÃ£o encontrado")
        
        # AnÃ¡lise com IA
        competitiveness_prompt = f"""
        Analise a competitividade desta licitaÃ§Ã£o:
        
        Objeto: {tender.object}
        Valor Estimado: {tender.estimated_value}
        Modalidade: {tender.modality}
        Dados de Mercado: {market_data or 'NÃ£o disponÃ­vel'}
        
        ForneÃ§a anÃ¡lise em JSON com:
        - nivel_competitividade (ALTO/MÃ‰DIO/BAIXO)
        - fatores_favoraveis (lista)
        - fatores_desfavoraveis (lista)
        - recomendacao_participacao (boolean)
        - estrategia_sugerida (string)
        """
        
        analysis = await ai_service._call_ollama_api(competitiveness_prompt)
        parsed_analysis = await ai_service._safe_json_parse(analysis)
        
        # Salvar anÃ¡lise
        tender.competitiveness_analysis = parsed_analysis
        db_session.commit()
        
        return {
            "status": "success",
            "tender_id": tender_id,
            "analysis": parsed_analysis
        }
        
    finally:
        db_session.close()

async def _update_tender_data(
    db_session, 
    tender_id: int, 
    extracted_data: Any
):
    """Atualiza dados do tender com informaÃ§Ãµes extraÃ­das"""
    
    tender = db_session.query(Tender).filter(Tender.id == tender_id).first()
    if not tender:
        raise ValueError(f"Tender {tender_id} nÃ£o encontrado")
    
    # Mapear dados extraÃ­dos para campos do modelo
    general_info = getattr(extracted_data, 'general_info', {})
    
    if 'numero_licitacao' in general_info:
        tender.number = general_info['numero_licitacao']
    
    if 'objeto_licitacao' in general_info:
        tender.object = general_info['objeto_licitacao']
    
    if 'valor_estimado' in general_info:
        tender.estimated_value = general_info['valor_estimado']
    
    if 'data_abertura' in general_info:
        tender.opening_date = general_info['data_abertura']
    
    # Armazenar dados completos da IA em campo JSONB
    tender.ai_extracted_data = {
        "general_info": general_info,
        "delivery_info": getattr(extracted_data, 'delivery_info', {}),
        "participation_conditions": getattr(extracted_data, 'participation_conditions', {}),
        "qualification_requirements": getattr(extracted_data, 'qualification_requirements', {}),
        "risk_analysis": getattr(extracted_data, 'risk_analysis', {}),
    }
    
    # Calcular score de risco
    risk_analysis = getattr(extracted_data, 'risk_analysis', {})
    tender.risk_score = _calculate_risk_score(risk_analysis)

async def _create_tender_items(
    db_session, 
    tender_id: int, 
    quotation_structure: Dict[str, Any]
):
    """Cria itens do tender baseados na estrutura de cotaÃ§Ã£o"""
    
    if 'planilha_cotacao' not in quotation_structure:
        return
    
    items_data = quotation_structure['planilha_cotacao'].get('itens', [])
    
    for item_data in items_data:
        tender_item = TenderItem(
            tender_id=tender_id,
            item_number=item_data.get('item_numero'),
            description=item_data.get('descricao'),
            quantity=item_data.get('quantidade'),
            unit=item_data.get('unidade'),
            specifications=item_data.get('especificacoes_tecnicas', []),
            ai_generated=True,
            created_by_ai=True
        )
        
        db_session.add(tender_item)

def _calculate_risk_score(risk_analysis: Dict[str, Any]) -> float:
    """Calcula score de risco baseado na anÃ¡lise da IA"""
    
    if not risk_analysis:
        return 0.5  # Neutro
    
    # Contagem de riscos por nÃ­vel
    alto_count = 0
    medio_count = 0
    baixo_count = 0
    
    for risk_category in ['riscos_prazo', 'riscos_financeiros', 'riscos_tecnicos', 'riscos_juridicos']:
        risks = risk_analysis.get(risk_category, [])
        for risk in risks:
            nivel = risk.get('nivel_risco', 'MÃ‰DIO')
            if nivel == 'ALTO':
                alto_count += 1
            elif nivel == 'MÃ‰DIO':
                medio_count += 1
            else:
                baixo_count += 1
    
    # CÃ¡lculo ponderado (0.0 = muito baixo risco, 1.0 = muito alto risco)
    total_risks = alto_count + medio_count + baixo_count
    if total_risks == 0:
        return 0.3  # Baixo risco se nÃ£o hÃ¡ riscos identificados
    
    weighted_score = (alto_count * 0.8 + medio_count * 0.5 + baixo_count * 0.2) / total_risks
    return min(weighted_score, 1.0)

def _calculate_overall_risk(extracted_data: Any) -> str:
    """Determina nÃ­vel de risco geral"""
    
    risk_analysis = getattr(extracted_data, 'risk_analysis', {})
    risk_score = _calculate_risk_score(risk_analysis)
    
    if risk_score >= 0.7:
        return "ALTO"
    elif risk_score >= 0.4:
        return "MÃ‰DIO"
    else:
        return "BAIXO"

async def _log_ai_processing(
    db_session,
    tender_id: int,
    user_id: int,
    extracted_data: Any,
    quotation_structure: Dict[str, Any],
    dispute_tracking: Dict[str, Any]
):
    """Log detalhado do processamento da IA"""
    
    log_entry = AIProcessingLog(
        tender_id=tender_id,
        user_id=user_id,
        processing_type="full_extraction",
        status="completed",
        input_data={
            "extraction_types": ["general_info", "delivery_info", "participation_conditions", 
                               "qualification_requirements", "risk_analysis", "reference_terms"]
        },
        output_data={
            "extracted_data": extracted_data.dict() if hasattr(extracted_data, 'dict') else extracted_data,
            "quotation_structure": quotation_structure,
            "dispute_tracking": dispute_tracking
        },
        model_used="llama3",
        processing_time_seconds=0,  # Seria calculado no contexto real
        created_at=db_session.now()
    )
    
    db_session.add(log_entry)

def _log_processing_error(tender_id: int, user_id: int, error_message: str):
    """Log de erros de processamento"""
    
    db_session = SessionLocal()
    try:
        log_entry = AIProcessingLog(
            tender_id=tender_id,
            user_id=user_id,
            processing_type="full_extraction",
            status="error",
            error_message=error_message,
            created_at=db_session.now()
        )
        
        db_session.add(log_entry)
        db_session.commit()
        
    finally:
        db_session.close()
```
          ### Monitoramento e Observabilidade

```python
# app/services/ai_monitoring_service.py

import logging
import time
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from collections import defaultdict, deque
import asyncio
from dataclasses import dataclass

from app.core.redis import redis_client
from app.core.settings import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)

@dataclass
class AIMetric:
    timestamp: float
    model: str
    operation: str
    processing_time: float
    success: bool
    error_type: Optional[str] = None
    input_size: int = 0
    output_size: int = 0

class AIMonitoringService:
    """ServiÃ§o de monitoramento e mÃ©tricas da IA"""
    
    def __init__(self):
        self.metrics_buffer = deque(maxlen=1000)
        self.rate_limiter = defaultdict(deque)
        
    async def record_metric(
        self,
        model: str,
        operation: str,
        processing_time: float,
        success: bool,
        error_type: Optional[str] = None,
        input_size: int = 0,
        output_size: int = 0
    ):
        """Registra mÃ©trica de operaÃ§Ã£o da IA"""
        
        metric = AIMetric(
            timestamp=time.time(),
            model=model,
            operation=operation,
            processing_time=processing_time,
            success=success,
            error_type=error_type,
            input_size=input_size,
            output_size=output_size
        )
        
        self.metrics_buffer.append(metric)
        
        # Persistir mÃ©tricas no Redis
        await self._persist_metric(metric)
        
        # Alertas automÃ¡ticos
        await self._check_alerts(metric)
    
    async def _persist_metric(self, metric: AIMetric):
        """Persiste mÃ©trica no Redis com TTL"""
        
        key = f"ai_metric:{int(metric.timestamp)}"
        data = {
            "model": metric.model,
            "operation": metric.operation,
            "processing_time": metric.processing_time,
            "success": metric.success,
            "error_type": metric.error_type or "",
            "input_size": metric.input_size,
            "output_size": metric.output_size
        }
        
        await redis_client.hset(key, mapping=data)
        await redis_client.expire(key, 86400 * 7)  # 7 dias
    
    async def _check_alerts(self, metric: AIMetric):
        """Verifica condiÃ§Ãµes de alerta"""
        
        # Alerta por tempo de processamento
        if metric.processing_time > settings.AI_RESPONSE_TIME_THRESHOLD:
            logger.warning(
                f"AI response time alert: {metric.processing_time:.2f}s "
                f"for {metric.operation} with {metric.model}"
            )
        
        # Alerta por taxa de erro
        recent_metrics = [m for m in self.metrics_buffer 
                         if m.timestamp > time.time() - 300]  # Ãºltimos 5 min
        
        if len(recent_metrics) >= 10:
            error_rate = sum(1 for m in recent_metrics if not m.success) / len(recent_metrics)
            if error_rate > 0.3:  # 30% de erro
                logger.error(f"High AI error rate detected: {error_rate:.2%}")
    
    async def get_metrics_summary(
        self, 
        time_range_minutes: int = 60
    ) -> Dict[str, Any]:
        """ObtÃ©m resumo de mÃ©tricas do perÃ­odo"""
        
        cutoff_time = time.time() - (time_range_minutes * 60)
        recent_metrics = [m for m in self.metrics_buffer if m.timestamp > cutoff_time]
        
        if not recent_metrics:
            return {"error": "No metrics available"}
        
        success_count = sum(1 for m in recent_metrics if m.success)
        total_count = len(recent_metrics)
        avg_processing_time = sum(m.processing_time for m in recent_metrics) / total_count
        
        # MÃ©tricas por modelo
        by_model = defaultdict(list)
        for metric in recent_metrics:
            by_model[metric.model].append(metric)
        
        model_stats = {}
        for model, metrics in by_model.items():
            model_stats[model] = {
                "total_requests": len(metrics),
                "success_rate": sum(1 for m in metrics if m.success) / len(metrics),
                "avg_processing_time": sum(m.processing_time for m in metrics) / len(metrics)
            }
        
        return {
            "time_range_minutes": time_range_minutes,
            "total_requests": total_count,
            "success_rate": success_count / total_count,
            "avg_processing_time": avg_processing_time,
            "by_model": model_stats,
            "recent_errors": [
                {"error_type": m.error_type, "timestamp": m.timestamp} 
                for m in recent_metrics if not m.success
            ][-5:]  # Ãºltimos 5 erros
        }
    
    async def check_rate_limit(self, user_id: int) -> bool:
        """Verifica rate limiting por usuÃ¡rio"""
        
        now = time.time()
        minute_window = now - 60
        hour_window = now - 3600
        
        # Limpar janelas antigas
        user_requests = self.rate_limiter[user_id]
        while user_requests and user_requests[0] < hour_window:
            user_requests.popleft()
        
        requests_last_minute = sum(1 for req_time in user_requests if req_time > minute_window)
        requests_last_hour = len(user_requests)
        
        # Verificar limites
        if requests_last_minute >= settings.AI_RATE_LIMIT_PER_MINUTE:
            return False
        
        if requests_last_hour >= settings.AI_RATE_LIMIT_PER_HOUR:
            return False
        
        # Registrar nova requisiÃ§Ã£o
        user_requests.append(now)
        return True

### Sistema de Health Checks

```python
# app/services/ai_health_service.py

import asyncio
import logging
from typing import Dict, Any, Optional
import httpx
from datetime import datetime

from app.core.settings import get_settings
from app.services.ai_processing_service import AIProcessingService

settings = get_settings()
logger = logging.getLogger(__name__)

class AIHealthService:
    """ServiÃ§o de health check para componentes de IA"""
    
    def __init__(self):
        self.ai_service = AIProcessingService()
    
    async def check_ollama_health(self) -> Dict[str, Any]:
        """Verifica saÃºde do serviÃ§o Ollama"""
        
        try:
            async with httpx.AsyncClient(
                base_url=settings.OLLAMA_API_URL,
                timeout=10.0
            ) as client:
                # Teste de conectividade
                response = await client.get("/")
                
                if response.status_code == 200:
                    # Teste de modelo
                    model_test = await self._test_model_availability()
                    return {
                        "status": "healthy",
                        "ollama_server": "connected",
                        "model_test": model_test,
                        "timestamp": datetime.utcnow().isoformat()
                    }
                else:
                    return {
                        "status": "unhealthy",
                        "error": f"Ollama returned status {response.status_code}",
                        "timestamp": datetime.utcnow().isoformat()
                    }
                    
        except httpx.ConnectError:
            return {
                "status": "unhealthy",
                "error": "Cannot connect to Ollama server",
                "timestamp": datetime.utcnow().isoformat()
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": f"Unexpected error: {str(e)}",
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def _test_model_availability(self) -> Dict[str, Any]:
        """Testa disponibilidade do modelo"""
        
        try:
            test_prompt = "Teste de conectividade. Responda apenas: OK"
            response = await self.ai_service._call_ollama_api(
                test_prompt,
                format_json=False
            )
            
            if "OK" in response.upper():
                return {
                    "status": "available",
                    "model": settings.OLLAMA_DEFAULT_MODEL,
                    "response_time": "< 5s"
                }
            else:
                return {
                    "status": "responding_incorrectly",
                    "model": settings.OLLAMA_DEFAULT_MODEL,
                    "response": response[:100]
                }
                
        except Exception as e:
            return {
                "status": "unavailable",
                "model": settings.OLLAMA_DEFAULT_MODEL,
                "error": str(e)
            }
    
    async def run_comprehensive_health_check(self) -> Dict[str, Any]:
        """Executa verificaÃ§Ã£o completa de saÃºde"""
        
        results = {
            "overall_status": "unknown",
            "timestamp": datetime.utcnow().isoformat(),
            "checks": {}
        }
        
        # Verificar Ollama
        ollama_health = await self.check_ollama_health()
        results["checks"]["ollama"] = ollama_health
        
        # Verificar GPU (se disponÃ­vel)
        gpu_status = await self._check_gpu_availability()
        results["checks"]["gpu"] = gpu_status
        
        # Verificar dependÃªncias
        deps_status = self._check_dependencies()
        results["checks"]["dependencies"] = deps_status
        
        # Status geral
        all_healthy = all(
            check.get("status") == "healthy" 
            for check in results["checks"].values()
        )
        
        results["overall_status"] = "healthy" if all_healthy else "unhealthy"
        
        return results
    
    async def _check_gpu_availability(self) -> Dict[str, Any]:
        """Verifica disponibilidade da GPU"""
        
        try:
            # Simular verificaÃ§Ã£o de GPU via Ollama
            async with httpx.AsyncClient(
                base_url=settings.OLLAMA_API_URL,
                timeout=5.0
            ) as client:
                response = await client.get("/api/ps")
                
                if response.status_code == 200:
                    models = response.json()
                    gpu_info = {
                        "status": "available",
                        "loaded_models": len(models.get("models", [])),
                        "models": [m.get("name") for m in models.get("models", [])]
                    }
                    return gpu_info
                
        except Exception as e:
            return {
                "status": "unknown",
                "error": str(e)
            }
        
        return {"status": "not_available"}
    
    def _check_dependencies(self) -> Dict[str, Any]:
        """Verifica dependÃªncias crÃ­ticas"""
        
        dependencies = {
            "httpx": False,
            "pymupdf": False,
            "python-docx": False,
            "pillow": False,
            "pytesseract": False
        }
        
        try:
            import httpx
            dependencies["httpx"] = True
        except ImportError:
            pass
        
        try:
            import fitz
            dependencies["pymupdf"] = True
        except ImportError:
            pass
        
        try:
            import docx
            dependencies["python-docx"] = True
        except ImportError:
            pass
        
        try:
            import PIL
            dependencies["pillow"] = True
        except ImportError:
            pass
        
        try:
            import pytesseract
            dependencies["pytesseract"] = True
        except ImportError:
            pass
        
        all_available = all(dependencies.values())
        
        return {
            "status": "healthy" if all_available else "missing_dependencies",
            "dependencies": dependencies,
            "missing": [name for name, available in dependencies.items() if not available]
        }

### Docker e Infraestrutura Otimizada